{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "    Project 1: N-Grams Language Model\n",
    "\n",
    "    Project for Amirkabir University of Technilogy (Tehran Polytechnic), Computer Scince department\n",
    "    Natural Language Processing course\n",
    "\n",
    "Student Name & ID: Pouria Alimoradpor 403112088\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = {\n",
    "    'Tasnim': './Datasets/Tasnim/tasnim.csv',\n",
    "    'Shahnameh': './Datasets/Shahnameh/shahname.csv',\n",
    "    'Hamshahri': './Datasets/HamshahriData/HamshahriCorpus'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data: Tasnim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø³ÛŒØ§Ø³ÛŒ</td>\n",
       "      <td>Ù…ÛŒØ±Ú©Ø§Ø¸Ù…ÛŒ: Ø²Ù…Ø§Ù† Ø­Ø°Ù Ø§Ø±Ø² Û´Û²Û°Û° ØªÙˆÙ…Ø§Ù†ÛŒ Ù…Ø´Ø®Øµ Ù†ÛŒØ³Øª</td>\n",
       "      <td>Ø±Ø¦ÛŒØ³ Ø³Ø§Ø²Ù…Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ùˆ Ø¨ÙˆØ¯Ø¬Ù‡ Ú¯ÙØª: Ù‡Ø± Ø²Ù…Ø§Ù† Ø´Ø±Ø§ÛŒØ·...</td>\n",
       "      <td>Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ú¯Ø±ÙˆÙ‡ Ù¾Ø§Ø±Ù„Ù…Ø§Ù†ÛŒ  ØŒ Â«Ù…Ø³Ø¹ÙˆØ¯ Ù…ÛŒØ±Ú©Ø§Ø¸Ù…ÛŒÂ» Ø±Ø¦ÛŒ...</td>\n",
       "      <td>Û²Û´ ÙØ±ÙˆØ±Ø¯ÙŠÙ† Û±Û´Û°Û± - Û°Û¹:ÛµÛ°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø³ÛŒØ§Ø³ÛŒ</td>\n",
       "      <td>Ø·Ø±Ø­ Û² ÙÙˆØ±ÛŒØªÛŒ Ø´ÙØ§ÙÛŒØª Ù‚ÙˆØ§ÛŒ Ø³Ù‡â€ŒÚ¯Ø§Ù†Ù‡ Ø¨Ø§ Û²Û°Û° Ø§Ù…Ø¶Ø§ ...</td>\n",
       "      <td>Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡ Ù†ÛŒØ´Ø§Ø¨ÙˆØ± Ø¯Ø± Ù…Ø¬Ù„Ø³ Ø§Ø² Ø§Ø±Ø§Ø¦Ù‡ Ø·Ø±Ø­ Û² ÙÙˆØ±ÛŒØªÛŒ...</td>\n",
       "      <td>Ø§Ø­Ø³Ø§Ù† Ø§Ø±Ú©Ø§Ù†ÛŒ Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡ Ù…Ø±Ø¯Ù… Ù†ÛŒØ´Ø§Ø¨ÙˆØ± Ø¯Ø± Ù…Ø¬Ù„Ø³ Ø´ÙˆØ±Ø§...</td>\n",
       "      <td>Û²Û´ ÙØ±ÙˆØ±Ø¯ÙŠÙ† Û±Û´Û°Û± - Û°Û¹:ÛµÛ°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø³ÛŒØ§Ø³ÛŒ</td>\n",
       "      <td>Ø±Ø¦ÛŒØ³ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø®Ø³Øª ÙˆØ²ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø±Ø§ ØªØ¨Ø±ÛŒÚ© ...</td>\n",
       "      <td>Ø±Ø¦ÛŒØ³ Ø¬Ù…Ù‡ÙˆØ± Ú©Ø´ÙˆØ±Ù…Ø§Ù† Ø·ÛŒ Ù¾ÛŒØ§Ù…ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø®Ø³Øª ÙˆØ²ÛŒØ± ...</td>\n",
       "      <td>Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø­ÙˆØ²Ù‡ Ø¯ÙˆÙ„Øª  ØŒ Ø¢ÛŒØªâ€ŒØ§Ù„Ù„Ù‡ Ø³ÛŒØ¯ Ø§Ø¨Ø±Ø§Ù‡ÛŒÙ… Ø±Ø¦ÛŒ...</td>\n",
       "      <td>Û²Ûµ ÙØ±ÙˆØ±Ø¯ÙŠÙ† Û±Û´Û°Û± - Û±Û³:ÛµÛ´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø³ÛŒØ§Ø³ÛŒ</td>\n",
       "      <td>Ø§ØµÙ„Ø§Ø­ Ø§Ø³Ø§Ø³Ù†Ø§Ù…Ù‡ Ø´Ø±Ú©Øª Ø´Ù‡Ø± ÙØ±ÙˆØ¯Ú¯Ø§Ù‡ÛŒ Ø§Ù…Ø§Ù… Ø®Ù…ÛŒÙ†ÛŒ (...</td>\n",
       "      <td>Ø§Ø³Ø§Ø³Ù†Ø§Ù…Ù‡ Ø´Ø±Ú©Øª Ø´Ù‡Ø± ÙØ±ÙˆØ¯Ú¯Ø§Ù‡ÛŒ Ø§Ù…Ø§Ù… Ø®Ù…ÛŒÙ†ÛŒ(Ø±Ù‡) Ø¯Ø± ...</td>\n",
       "      <td>Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø­ÙˆØ²Ù‡ Ø¯ÙˆÙ„Øª  ØŒ Ø¯Ø± Ø¬Ù„Ø³Ù‡ ØµØ¨Ø­ Ø±ÙˆØ² Ú†Ù‡Ø§Ø±Ø´Ù†Ø¨Ù‡...</td>\n",
       "      <td>Û³Û± ÙØ±ÙˆØ±Ø¯ÙŠÙ† Û±Û´Û°Û± - Û±Û²:Û°Û³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø³ÛŒØ§Ø³ÛŒ</td>\n",
       "      <td>Ø§Ø±Ø§Ø¦Ù‡ Ø·Ø±Ø­ÛŒ Ú©Ù„ÛŒ Ùˆ Ù…Ø¨Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ø´ÙØ§ÙÛŒØª ÙØ±Ø§Ø± Ø§Ø² Ù…Ø·Ø§...</td>\n",
       "      <td>Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡ Ù…Ø±Ø¯Ù… ØªÙ‡Ø±Ø§Ù† Ú¯ÙØª: Ø¯Ø± Ø´Ø±Ø§ÛŒØ·ÛŒ Ú©Ù‡ Ø·Ø±Ø­ Ø´ÙØ§Ù...</td>\n",
       "      <td>Ø¹Ù„ÛŒ Ø®Ø¶Ø±ÛŒØ§Ù† Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡ ØªÙ‡Ø±Ø§Ù† Ø¯Ø± Ù…Ø¬Ù„Ø³ Ø¯Ø± Ú¯ÙØªâ€ŒÙˆÚ¯Ùˆ Ø¨Ø§...</td>\n",
       "      <td>Û³Û° ÙØ±ÙˆØ±Ø¯ÙŠÙ† Û±Û´Û°Û± - Û±Û¶:Û±Ûµ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                              title  \\\n",
       "0    Ø³ÛŒØ§Ø³ÛŒ      Ù…ÛŒØ±Ú©Ø§Ø¸Ù…ÛŒ: Ø²Ù…Ø§Ù† Ø­Ø°Ù Ø§Ø±Ø² Û´Û²Û°Û° ØªÙˆÙ…Ø§Ù†ÛŒ Ù…Ø´Ø®Øµ Ù†ÛŒØ³Øª    \n",
       "1    Ø³ÛŒØ§Ø³ÛŒ   Ø·Ø±Ø­ Û² ÙÙˆØ±ÛŒØªÛŒ Ø´ÙØ§ÙÛŒØª Ù‚ÙˆØ§ÛŒ Ø³Ù‡â€ŒÚ¯Ø§Ù†Ù‡ Ø¨Ø§ Û²Û°Û° Ø§Ù…Ø¶Ø§ ...   \n",
       "2    Ø³ÛŒØ§Ø³ÛŒ   Ø±Ø¦ÛŒØ³ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø®Ø³Øª ÙˆØ²ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø±Ø§ ØªØ¨Ø±ÛŒÚ© ...   \n",
       "3    Ø³ÛŒØ§Ø³ÛŒ   Ø§ØµÙ„Ø§Ø­ Ø§Ø³Ø§Ø³Ù†Ø§Ù…Ù‡ Ø´Ø±Ú©Øª Ø´Ù‡Ø± ÙØ±ÙˆØ¯Ú¯Ø§Ù‡ÛŒ Ø§Ù…Ø§Ù… Ø®Ù…ÛŒÙ†ÛŒ (...   \n",
       "4    Ø³ÛŒØ§Ø³ÛŒ   Ø§Ø±Ø§Ø¦Ù‡ Ø·Ø±Ø­ÛŒ Ú©Ù„ÛŒ Ùˆ Ù…Ø¨Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ø´ÙØ§ÙÛŒØª ÙØ±Ø§Ø± Ø§Ø² Ù…Ø·Ø§...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0   Ø±Ø¦ÛŒØ³ Ø³Ø§Ø²Ù…Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ùˆ Ø¨ÙˆØ¯Ø¬Ù‡ Ú¯ÙØª: Ù‡Ø± Ø²Ù…Ø§Ù† Ø´Ø±Ø§ÛŒØ·...   \n",
       "1   Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡ Ù†ÛŒØ´Ø§Ø¨ÙˆØ± Ø¯Ø± Ù…Ø¬Ù„Ø³ Ø§Ø² Ø§Ø±Ø§Ø¦Ù‡ Ø·Ø±Ø­ Û² ÙÙˆØ±ÛŒØªÛŒ...   \n",
       "2   Ø±Ø¦ÛŒØ³ Ø¬Ù…Ù‡ÙˆØ± Ú©Ø´ÙˆØ±Ù…Ø§Ù† Ø·ÛŒ Ù¾ÛŒØ§Ù…ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø®Ø³Øª ÙˆØ²ÛŒØ± ...   \n",
       "3   Ø§Ø³Ø§Ø³Ù†Ø§Ù…Ù‡ Ø´Ø±Ú©Øª Ø´Ù‡Ø± ÙØ±ÙˆØ¯Ú¯Ø§Ù‡ÛŒ Ø§Ù…Ø§Ù… Ø®Ù…ÛŒÙ†ÛŒ(Ø±Ù‡) Ø¯Ø± ...   \n",
       "4   Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡ Ù…Ø±Ø¯Ù… ØªÙ‡Ø±Ø§Ù† Ú¯ÙØª: Ø¯Ø± Ø´Ø±Ø§ÛŒØ·ÛŒ Ú©Ù‡ Ø·Ø±Ø­ Ø´ÙØ§Ù...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ú¯Ø±ÙˆÙ‡ Ù¾Ø§Ø±Ù„Ù…Ø§Ù†ÛŒ  ØŒ Â«Ù…Ø³Ø¹ÙˆØ¯ Ù…ÛŒØ±Ú©Ø§Ø¸Ù…ÛŒÂ» Ø±Ø¦ÛŒ...   \n",
       "1  Ø§Ø­Ø³Ø§Ù† Ø§Ø±Ú©Ø§Ù†ÛŒ Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡ Ù…Ø±Ø¯Ù… Ù†ÛŒØ´Ø§Ø¨ÙˆØ± Ø¯Ø± Ù…Ø¬Ù„Ø³ Ø´ÙˆØ±Ø§...   \n",
       "2  Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø­ÙˆØ²Ù‡ Ø¯ÙˆÙ„Øª  ØŒ Ø¢ÛŒØªâ€ŒØ§Ù„Ù„Ù‡ Ø³ÛŒØ¯ Ø§Ø¨Ø±Ø§Ù‡ÛŒÙ… Ø±Ø¦ÛŒ...   \n",
       "3  Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø­ÙˆØ²Ù‡ Ø¯ÙˆÙ„Øª  ØŒ Ø¯Ø± Ø¬Ù„Ø³Ù‡ ØµØ¨Ø­ Ø±ÙˆØ² Ú†Ù‡Ø§Ø±Ø´Ù†Ø¨Ù‡...   \n",
       "4  Ø¹Ù„ÛŒ Ø®Ø¶Ø±ÛŒØ§Ù† Ù†Ù…Ø§ÛŒÙ†Ø¯Ù‡ ØªÙ‡Ø±Ø§Ù† Ø¯Ø± Ù…Ø¬Ù„Ø³ Ø¯Ø± Ú¯ÙØªâ€ŒÙˆÚ¯Ùˆ Ø¨Ø§...   \n",
       "\n",
       "                         time  \n",
       "0    Û²Û´ ÙØ±ÙˆØ±Ø¯ÙŠÙ† Û±Û´Û°Û± - Û°Û¹:ÛµÛ°   \n",
       "1    Û²Û´ ÙØ±ÙˆØ±Ø¯ÙŠÙ† Û±Û´Û°Û± - Û°Û¹:ÛµÛ°   \n",
       "2    Û²Ûµ ÙØ±ÙˆØ±Ø¯ÙŠÙ† Û±Û´Û°Û± - Û±Û³:ÛµÛ´   \n",
       "3    Û³Û± ÙØ±ÙˆØ±Ø¯ÙŠÙ† Û±Û´Û°Û± - Û±Û²:Û°Û³   \n",
       "4    Û³Û° ÙØ±ÙˆØ±Ø¯ÙŠÙ† Û±Û´Û°Û± - Û±Û¶:Û±Ûµ   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tasnim = pd.read_csv(data_path['Tasnim'], encoding='utf-8')\n",
    "data_tasnim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63494 entries, 0 to 63493\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  63494 non-null  object\n",
      " 1   title     63493 non-null  object\n",
      " 2   abstract  63493 non-null  object\n",
      " 3   body      62558 non-null  object\n",
      " 4   time      63493 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data_tasnim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù…ÛŒØ±Ú©Ø§Ø¸Ù…ÛŒ: Ø²Ù…Ø§Ù† Ø­Ø°Ù Ø§Ø±Ø² Û´Û²Û°Û° ØªÙˆÙ…Ø§Ù†ÛŒ Ù…Ø´Ø®Øµ Ù†ÛŒØ³Øª ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø·Ø±Ø­ Û² ÙÙˆØ±ÛŒØªÛŒ Ø´ÙØ§ÙÛŒØª Ù‚ÙˆØ§ÛŒ Ø³Ù‡â€ŒÚ¯Ø§Ù†Ù‡ Ø¨Ø§ Û²Û°Û° Ø§Ù…Ø¶Ø§ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø±Ø¦ÛŒØ³ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø®Ø³Øª ÙˆØ²ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø±Ø§ ØªØ¨Ø±ÛŒÚ© ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø§ØµÙ„Ø§Ø­ Ø§Ø³Ø§Ø³Ù†Ø§Ù…Ù‡ Ø´Ø±Ú©Øª Ø´Ù‡Ø± ÙØ±ÙˆØ¯Ú¯Ø§Ù‡ÛŒ Ø§Ù…Ø§Ù… Ø®Ù…ÛŒÙ†ÛŒ (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø§Ø±Ø§Ø¦Ù‡ Ø·Ø±Ø­ÛŒ Ú©Ù„ÛŒ Ùˆ Ù…Ø¨Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ø´ÙØ§ÙÛŒØª ÙØ±Ø§Ø± Ø§Ø² Ù…Ø·Ø§...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0   Ù…ÛŒØ±Ú©Ø§Ø¸Ù…ÛŒ: Ø²Ù…Ø§Ù† Ø­Ø°Ù Ø§Ø±Ø² Û´Û²Û°Û° ØªÙˆÙ…Ø§Ù†ÛŒ Ù…Ø´Ø®Øµ Ù†ÛŒØ³Øª ...\n",
       "1   Ø·Ø±Ø­ Û² ÙÙˆØ±ÛŒØªÛŒ Ø´ÙØ§ÙÛŒØª Ù‚ÙˆØ§ÛŒ Ø³Ù‡â€ŒÚ¯Ø§Ù†Ù‡ Ø¨Ø§ Û²Û°Û° Ø§Ù…Ø¶Ø§ ...\n",
       "2   Ø±Ø¦ÛŒØ³ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø®Ø³Øª ÙˆØ²ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø±Ø§ ØªØ¨Ø±ÛŒÚ© ...\n",
       "3   Ø§ØµÙ„Ø§Ø­ Ø§Ø³Ø§Ø³Ù†Ø§Ù…Ù‡ Ø´Ø±Ú©Øª Ø´Ù‡Ø± ÙØ±ÙˆØ¯Ú¯Ø§Ù‡ÛŒ Ø§Ù…Ø§Ù… Ø®Ù…ÛŒÙ†ÛŒ (...\n",
       "4   Ø§Ø±Ø§Ø¦Ù‡ Ø·Ø±Ø­ÛŒ Ú©Ù„ÛŒ Ùˆ Ù…Ø¨Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ø´ÙØ§ÙÛŒØª ÙØ±Ø§Ø± Ø§Ø² Ù…Ø·Ø§..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tasnim = data_tasnim.drop(columns=['category', 'time'])\n",
    "data_tasnim['text'] = data_tasnim['title'] + '. ' + data_tasnim['abstract'] + ' ' + data_tasnim['body']\n",
    "data_tasnim = data_tasnim.drop(columns=['title', 'abstract', 'body'])\n",
    "data_tasnim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ù…ÛŒØ±Ú©Ø§Ø¸Ù…ÛŒ: Ø²Ù…Ø§Ù† Ø­Ø°Ù Ø§Ø±Ø² Û´Û²Û°Û° ØªÙˆÙ…Ø§Ù†ÛŒ Ù…Ø´Ø®Øµ Ù†ÛŒØ³Øª .  Ø±Ø¦ÛŒØ³ Ø³Ø§Ø²Ù…Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ùˆ Ø¨ÙˆØ¯Ø¬Ù‡ Ú¯ÙØª: Ù‡Ø± Ø²Ù…Ø§Ù† Ø´Ø±Ø§ÛŒØ· Ùˆ ÙØ¶Ø§ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ú©Ø´ÙˆØ± Ù…Ø³Ø§Ø¹Ø¯ Ø¨Ø§Ø´Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡ ØªØ¯Ø±ÛŒØ¬ Ø­Ø°Ù Ø§Ø±Ø² ØªØ±Ø¬ÛŒØ­ÛŒ Ø±Ø§ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ Ú©Ø±Ø¯ Ùˆ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ø§Ø±Ø² Û´Û²Û°Û° ØªÙˆÙ…Ø§Ù†ÛŒ Ø²Ù…Ø§Ù† Ùˆ ØªØ§Ø±ÛŒØ® Ù…Ø´Ø®ØµÛŒ Ø±Ø§ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† ØªØ¹ÛŒÛŒÙ† Ú©Ø±Ø¯.  Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ú¯Ø±ÙˆÙ‡ Ù¾Ø§Ø±Ù„Ù…Ø§Ù†ÛŒ  ØŒ Â«Ù…Ø³Ø¹ÙˆØ¯ Ù…ÛŒØ±Ú©Ø§Ø¸Ù…ÛŒÂ» Ø±Ø¦ÛŒØ³ Ø³Ø§Ø²Ù…Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ùˆ Ø¨ÙˆØ¯Ø¬Ù‡ØŒ Ø¯Ø± Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³Ø¤Ø§Ù„ÛŒ Ù…Ø¨Ù†ÛŒ Ø¨Ø± Ø§ÛŒÙ†Ú©Ù‡ Ø¯ÙˆÙ„Øª Ø§Ø² Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ù‚Ø¯Ø§Ù… Ø¨Ù‡ Ø­Ø°Ù Ø§Ø±Ø² 4200 ØªÙˆÙ…Ø§Ù†ÛŒ Ø®ÙˆØ§Ù‡Ø¯ Ú©Ø±Ø¯ØŒÂ Ú¯ÙØª: Ø¨Ø±Ø§Ø³Ø§Ø³ Ù‚Ø§Ù†ÙˆÙ† Ø¨ÙˆØ¯Ø¬Ù‡ Ø³Ø§Ù„ 1401ØŒ Ø§Ø®ØªÛŒØ§Ø± Ø­Ø°Ù Ùˆ ÛŒØ§ Ø¹Ø¯Ù… Ø­Ø°Ù Ø§Ø±Ø² ØªØ±Ø¬ÛŒØ­ÛŒ Ø¨Ø±Ø¹Ù‡Ø¯Ù‡ Ø¯ÙˆÙ„Øª Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª ÙˆÂ Ø§Ø² Ø³ÙˆÛŒ Ø¯ÛŒÚ¯Ø± Ø­Ø¬Ù… Ø§Ø±Ø² 4200 ØªÙˆÙ…Ø§Ù†ÛŒ Ø¯Ø± Ú©Ø´ÙˆØ± Ø¨Ù‡ Ù…ÛŒØ²Ø§Ù†ÛŒ Ù†ÛŒØ³Øª Ú©Ù‡ Ø¨ØªÙˆØ§Ù† ØªØ§ Ù¾Ø§ÛŒØ§Ù† Ø³Ø§Ù„ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯. ÙˆÛŒ Ú©Ù‡ Ø¨Ø§ Ø®Ø§Ù†Ù‡ Ù…Ù„Øª Ú¯ÙØªâ€ŒÙˆÚ¯Ùˆ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø¯: Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ù‡Ø± Ø²Ù…Ø§Ù† Ø´Ø±Ø§ÛŒØ· Ùˆ ÙØ¶Ø§ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ú©Ø´ÙˆØ± Ù…Ø³Ø§Ø¹Ø¯ Ø¨Ø§Ø´Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡ ØªØ¯Ø±ÛŒØ¬ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ Ú©Ø±Ø¯Â Ùˆ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ø§Ø±Ø² ØªØ±Ø¬ÛŒØ­ÛŒ Ø²Ù…Ø§Ù† Ùˆ ØªØ§Ø±ÛŒØ® Ù…Ø´Ø®ØµÛŒ Ø±Ø§ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† ØªØ¹ÛŒÛŒÙ† Ú©Ø±Ø¯. Ø±Ø¦ÛŒØ³ Ø³Ø§Ø²Ù…Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ùˆ Ø¨ÙˆØ¯Ø¬Ù‡ Ø¯Ø± Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ø¯ÛŒÚ¯Ø±ÛŒ Ù…Ø¨Ù†ÛŒ Ø¨Ø± Ø§ÛŒÙ†Ú©Ù‡ Ú†Ø±Ø§ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§ÛŒÙ†Ú©Ù‡ Ù‡Ù†ÙˆØ² Ø§Ø±Ø² ØªØ±Ø¬ÛŒØ­ÛŒ Ø­Ø°Ù Ù†Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ Ø´Ø§Ù‡Ø¯ Ø§ÙØ²Ø§ÛŒØ´ Ù‚ÛŒÙ…Øª Ú©Ø§Ù„Ø§Ù‡Ø§ Ø¨Ù‡ ÙˆÛŒÚ˜Ù‡ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ú¯ÙˆØ´Øª Ù‚Ø±Ù…Ø² Ùˆ Ù…Ø±Øº Ø¯Ø± Ú©Ø´ÙˆØ± Ù‡Ø³ØªÛŒÙ…ØŒ Ø§Ø¸Ù‡Ø§Ø± Ú©Ø±Ø¯: Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ Ø±ÙˆÛŒ Ù†Ø±Ø® Ú¯ÙˆØ´Øª Ù‚Ø±Ù…Ø² Ùˆ Ù…Ø±Øº ØªØºÛŒÛŒØ±ÛŒ Ù†Ú©Ø±Ø¯Ù‡ Ø§Ø³ØªØ› Ø¨Ø±Ø®ÛŒ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ Ùˆ Ù‚ÛŒÙ…Øª ØªÙ…Ø§Ù… Ø´Ø¯Ù‡ Ù…ÙˆØ§Ø¯ ØºØ°Ø§ÛŒÛŒ Ø¨Ù‡ Ù†Ø±Ø® Ù…ÙˆØ§Ø¯ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø³ØªÚ¯ÛŒ Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ø¹ÙˆØ§Ù…Ù„ Ø¯ÛŒÚ¯Ø±ÛŒ Ù†ÛŒØ² Ø¯Ø± Ø§ÛŒÙ† Ù…ÛŒØ§Ù† Ù…Ø¤Ø«Ø± Ù‡Ø³ØªÙ†Ø¯Â Ùˆ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ØªØ£Ø«ÛŒØ±Ø§Øª Ø¨Ø§ÛŒØ¯ ÛŒØ§Ø±Ø§Ù†Ù‡ Ù¾Ø±Ø¯Ø§Ø®Øª Ø´ÙˆØ¯ Ùˆ ÛŒØ§ Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ø§Ø¯ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ Ø§ØµÙ„Ø§Ø­ Ø´ÙˆØ¯. Ø§Ù†ØªÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ù…/Ø›\n"
     ]
    }
   ],
   "source": [
    "print(data_tasnim['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ù‹', 'ï¼’', 'âœŒ', 'Å', '\\u2003', 'ï¼”', 'Û¸', 'ï®’', 'ïº¾', 'Ï', 'Û€', '\\u200d', '=', 'J', 'Ñ€', 'Ğ±', 'I', 'Ã¸', 'Û–', 'Ğº', 'ï»ˆ', 'ï®•', 'Ã‰', 'ïºµ', 'ï¼•', 'Ğ¡', 'Ú¾', 'Ãº', 'Ø¯', 'ï®”', 'ï»„', '\\u202c', 'ï­¼', 'Ä±', 'ïº¶', 'Ù”', 'ğŸ”¸', 'ï®', 'Î³', '^', 'Ûµ', 'Î²', 'e', 'Ã­', 'i', 'ï»š', 'y', 'âˆ’', 'Ğ²', 't', 'Ã¼', 'O', 'Ø²', 'Û—', 'ï»£', 'ï»Œ', 'Ğµ', 'ïº—', 'f', 'Ğ°', '+', 'ğŸ“±', 'Û™', 'r', 'Ï…', '\\u2069', 'â‰ˆ', 'Ù', '\\u200a', 'ïº»', 'ï´¾', '#', 'â€', 'Îº', 'Ù‚', 'ï®', 'ïº˜', 'Ú¼', 'âµ', 'Ø¢', '\\u2028', 'Û°', 'A', 'Ã³', 'Û³', 'p', 'Î¿', 'ï»¢', 'Ù«', 'Ù©', 'Î•', 'ğŸ‘ˆ', 'Ù´', '2', 'Ø¨', 'ï»¤', '|', '\\uf0d8', 'Ã†', 'n', 'ïº¤', '`', 'Î¹', 'Ø©', 'w', 'ï»“', 'ï»•', 'z', '\\x88', 'c', 'ïº', 'Ñ‹', 'Ú‹', '\\u202b', 'g', 'ï»›', 'ï»­', 'Ù‘', 'ï»»', 'ï®“', 'ï»', 'ï»†', 'ï»¨', 'Ã©', 'ÄŸ', 'ï»±', 'N', 'Î¡', 'q', 'â€¦', 'ï¯¾', '9', 'G', 'â€¼', 'â€¹', 'ï­½', 'ï­»', 'ï»', 'ï»¼', '\\u200b', 'ï»‘', 'Q', 'Øª', 'ï¼–', 'ï»¯', 'ï»˜', 'Ù•', 'Î½', 'd', 'Ãµ', 'ï®‘', 'ğŸ”´', 'ïº•', '3', 'ï»°', '[', 'ïºŒ', 'â¦', 'Û±', 'ïº¨', 'ïº', 'Ã¡', 'ïº£', 'ï»Š', 'Ã', 'Û•', 'ğŸŒ¹', 'Î±', 'Ø', 'ÙŒ', '!', 'ï»’', 'Û”', 'ï»³', 'ï»‰', '/', 'ï»‡', 'ïº¢', 'ïº²', 'Ø¹', 'ï»Ÿ', 'ï»¸', 'ï·²', 'ğŸ˜’', 'â˜‘', 'ïº¼', 'ïº', 'Ù¢', '\\x9d', 'ï»¡', 'R', '(', '\\u200e', 'Ø®', 'Ğ»', 'ïºœ', '\\xad', 'u', 'm', 'ïº°', 'ğŸ”·', 'W', 'Å™', 'Ïƒ', 'Ã—', 'â€œ', 'Úº', 'Ï‰', 'v', 'ï»–', \"'\", 'M', 'ğŸ˜‰', 'x', '4', 'Â»', 'â€“', 'B', ':', 'ğŸ”º', 'V', 'Ù‰', 'ØŸ', '>', '\\u2005', 'ï»œ', 'Ø¶', 'ïº¡', 'ï­˜', '*', 'ï»', 'Ï„', 'â›”', 'ğŸ”½', 'ï®', 'Ğ¢', 'Û²', '6', '_', 'ï»¦', 'ğŸ”°', 'ï»«', 'Ú…', 'U', 'ï»´', 'Î”', '.', 'Ù…', 'Ù', 'ğŸ¼', 'Ù', 'ï¸', 'â€”', 'ïº­', 'ïº§', 'Ø´', 'Ğ½', 'Î´', 'ï­™', 'Ø¡', 'Øº', 'Ûš', 'ïº¹', '\\u202a', '\\x83', 'ï¼—', 'âœ', 'Ø­', 'Ú¯', 'S', 'Â±', 'Ù±', 'ïº„', 'b', 'ğŸ”¹', 'Ù–', '\\u061c', 'â—', 'Â¤', '\\u2066', 'ğŸ’¢', 'ï»§', 'Ù¤', 'Ù²', 'Ä°', 'ğŸ‘‰', 'ïºª', 'ïº³', '~', '\\xa0', 'Ù¹', 'l', 'Ğ¸', 'Ùƒ', 'ïº¿', 'o', 'ïº‹', 'ï¼“', 'ï¼‘', 'Û¶', 'Ø¬', 'Ã¶', '\\u2063', 'Îµ', 'â€™', 'ïº', 'â–«', 'Â¡', '\\u200f', 'Ø«', 'Ğš', ';', 'ïº®', 'Y', 'ïºº', 'ĞŸ', 'Ø§', 'Ù¾', 'Ù§', 'Ù‡', 'â€¢', 'ï»‚', 'Ñ', 'ÛŒ', 'L', 'ïº‘', 'Ø›', 'Ï€', 'ï»©', 'ïº¯', 'F', 'Ù¡', 'Ø£', 'ï»ƒ', '?', 'Ø¤', 'Ğ¹', 'Ù¥', 'Ùˆ', 'Ù“', 'Ù¦', 'Ã´', 'Ã±', 'ïº©', '\"', 'D', '\\ufeff', 'ğŸ“·', ']', '5', 'ïº±', 'Ğ¾', 'ÙŠ', 'â€˜', 'Ù’', '&', '%', '@', 'ïº', '8', 'ï»²', 'Â¬', 'ã€‹', 'Ğ‘', 'Äƒ', 'Î¼', 'Î¯', 'Ú˜', 'Û', 'Ã§', 'Ø¸', '{', 'P', '$', 'k', 'Ä‡', 'Ù¬', ',', 'ï»', 'Ğ´', 'Ã®', 'ïºŸ', 'Ã–', 'E', 'Ñ‚', 'Ù°', 'ïº´', 'ïº·', 'Ù£', 'T', 'Â«', 'ï®‹', '7', 'Û¹', '<', 'ğŸ“¸', 'Ğ³', 'Øµ', 'H', 'a', 'ğŸ˜¢', 'ğŸ›‘', '-', 'Ğ¿', 'Ø¥', 'ï»”', 'ï¿½', 'ï¯¼', 'ï¯¿', 'ïº', 'ïº’', 'â€º', 'j', 'ï¯½', 'h', 'ï»‹', '\\u202d', 'ïº', 'ïº«', 'Ùª', 'Ù†', 'Ú†', '0', 'K', 'Û´', 'Ã£', ')', 'Ø¦', 'ï»', 'ïº–', 'C', 'Ä', '\\u2009', 'Ø³', 'Ø°', 'Ù¨', 'Ø·', 'ã€Š', 'Ãœ', ' ', 'Ù', 'ï»ª', 'X', 'Ñƒ', 'Ä“', 'ïº¸', 'Ù„', '\\x96', 'ï»', 'ï»', 'Ã·', 'ğŸ”»', 'Ø±', 'Ğ¼', 'Î¸', '\\u200c', 'Å¡', '1', 'ØŒ', 'Z', 'È›', 'ï»¬', 'ï»€', 'Û·', 'Ú©', 's', '\\ufff8', '\\\\', 'ÅŸ', 'Â·', '}', 'Ù€', 'ï»®', '\\u2067', 'ïº›', 'ï»¥', 'ïº¬', 'ï»—', 'â™¦', 'ï´¿', 'Ù', 'Ã¤'}\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set()\n",
    "for text in data_tasnim['text'].dropna():\n",
    "    unique_chars.update(set(text))\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data: Shahnameh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Part</th>\n",
       "      <th>Bait</th>\n",
       "      <th>Mesra</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¬Ø§Ù† Ùˆ Ø®Ø±Ø¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Ú©Ø² Ø§ÛŒÙ† Ø¨Ø±ØªØ± Ø§Ù†Ø¯ÛŒØ´Ù‡ Ø¨Ø± Ù†Ú¯Ø°Ø±Ø¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ù†Ø§Ù… Ùˆ Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¬Ø§ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø±ÙˆØ²ÛŒ Ø¯Ù‡ Ø±Ù‡Ù†Ù…Ø§ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ú©ÛŒÙˆØ§Ù† Ùˆ Ú¯ÙØ±Ø¯Ø§Ù† Ø³Ù¾Ù‡Ø±</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chapter  Part  Bait  Mesra                         Text\n",
       "0        1     1     1      1      Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¬Ø§Ù† Ùˆ Ø®Ø±Ø¯\n",
       "1        1     1     1      2  Ú©Ø² Ø§ÛŒÙ† Ø¨Ø±ØªØ± Ø§Ù†Ø¯ÛŒØ´Ù‡ Ø¨Ø± Ù†Ú¯Ø°Ø±Ø¯\n",
       "2        1     1     2      1      Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ù†Ø§Ù… Ùˆ Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¬Ø§ÛŒ\n",
       "3        1     1     2      2        Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø±ÙˆØ²ÛŒ Ø¯Ù‡ Ø±Ù‡Ù†Ù…Ø§ÛŒ\n",
       "4        1     1     3      1   Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ú©ÛŒÙˆØ§Ù† Ùˆ Ú¯ÙØ±Ø¯Ø§Ù† Ø³Ù¾Ù‡Ø±"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shahnameh = pd.read_csv(data_path['Shahnameh'], encoding='utf-8')\n",
    "data_shahnameh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99222 entries, 0 to 99221\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Chapter  99222 non-null  int64 \n",
      " 1   Part     99222 non-null  int64 \n",
      " 2   Bait     99222 non-null  int64 \n",
      " 3   Mesra    99222 non-null  int64 \n",
      " 4   Text     99222 non-null  object\n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data_shahnameh.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¬Ø§Ù† Ùˆ Ø®Ø±Ø¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú©Ø² Ø§ÛŒÙ† Ø¨Ø±ØªØ± Ø§Ù†Ø¯ÛŒØ´Ù‡ Ø¨Ø± Ù†Ú¯Ø°Ø±Ø¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ù†Ø§Ù… Ùˆ Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¬Ø§ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø±ÙˆØ²ÛŒ Ø¯Ù‡ Ø±Ù‡Ù†Ù…Ø§ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ú©ÛŒÙˆØ§Ù† Ùˆ Ú¯ÙØ±Ø¯Ø§Ù† Ø³Ù¾Ù‡Ø±</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text\n",
       "0      Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¬Ø§Ù† Ùˆ Ø®Ø±Ø¯\n",
       "1  Ú©Ø² Ø§ÛŒÙ† Ø¨Ø±ØªØ± Ø§Ù†Ø¯ÛŒØ´Ù‡ Ø¨Ø± Ù†Ú¯Ø°Ø±Ø¯\n",
       "2      Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ù†Ø§Ù… Ùˆ Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¬Ø§ÛŒ\n",
       "3        Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø±ÙˆØ²ÛŒ Ø¯Ù‡ Ø±Ù‡Ù†Ù…Ø§ÛŒ\n",
       "4   Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ú©ÛŒÙˆØ§Ù† Ùˆ Ú¯ÙØ±Ø¯Ø§Ù† Ø³Ù¾Ù‡Ø±"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shahnameh = data_shahnameh.drop(columns=['Chapter', 'Part', 'Bait', 'Mesra'])\n",
    "data_shahnameh = data_shahnameh.rename(columns={'Text': 'text'})\n",
    "data_shahnameh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ø¹', 'Ø¬', 'Ø²', 'Ù‘', 'Ù…', 'Ù†', 'Ù', 'Ú†', 'Ø«', ')', 'Ø¦', 'Ú˜', 'Ø´', 'Ø°', 'Ø³', 'Ø·', 'Ø¸', ' ', 'Ø¡', '(', 'Ù', 'Øº', 'Ø§', 'Ù', 'Ø®', 'Ù¾', 'Ù‡', 'Ù„', 'Ø­', 'Ù‚', 'ÛŒ', 'Ø›', 'Ú¯', 'Øª', 'Ø±', 'Ø¢', '\\u200c', 'Ø¯', 'Â«', 'Ù”', 'ØŒ', 'Â»', 'Øµ', 'Ø£', ':', 'Ø¤', 'Ùˆ', 'ØŸ', 'Ú©', 'Ø¶', 'Ø¨', '\\xa0', 'ÙŠ', '!', 'Ù’', 'Ù'}\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set()\n",
    "for text in data_shahnameh['text']:\n",
    "    unique_chars.update(set(text))\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data: Hamshahri**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hamshahri = []\n",
    "for year_folder in os.listdir(data_path['Hamshahri']):\n",
    "    folder_path = os.path.join(data_path['Hamshahri'], year_folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".ham\"):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data_hamshahri.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø¬ÙˆØ¯ÙŠ Ø§Ø¨ÙˆØªØŒ Ù†Ø§Ø¨ØºÙ‡ ÙƒØ§ØºØ°ÙŠ\\nØ³Ù…ÙŠÙ‡ Ù†ØµÙŠØ±ÙŠ Ù‡Ø§\\nØ¬ÙˆØ¯ÙŠ Ø§Ø¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø±ÙŠØ²ÙŠ Ø¨Ø±Ø§ÙŠ Ù…Ø±Ú¯\\nÙ†Ú¯Ø§Ù‡ÙŠ Ø¨Ù‡ Ø§Ø³ØªØ±Ø³ Ùˆ Ø³Ø§Ø² Ùˆ Ùƒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø¢ØºØ§Ø² Ø³Ø§Ø®Øª Ø¨Ø¯Ù†Ù‡ Ùˆ Ø³Ø±Ø±ÙŠØ² Ø¨Ù„Ù†Ø¯ØªØ±ÙŠÙ† Ø³Ø¯\\nÙƒØ´ÙˆØ±\\nØ¹Ù…Ù„ÙŠ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø¹ÙˆØ§Ù…Ù„ Ù…ÙˆØ«Ø± Ø¯Ø± Ù¾ÙŠØ´Ú¯ÙŠØ±ÙŠ Ø§Ø² Ø¯ÙŠØ§Ø¨Øª\\nØ¯ÙƒØªØ± Ù…Ø­Ù…ÙˆØ¯ Ø¨Ù‡Ø²...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø¢ØºØ§Ø² Ø¹Ù…Ù„ÙŠØ§Øª Ø³Ø§Ø®Øª Ù…ØªØ±Ùˆ ØªØ¬Ø±ÙŠØ´\\nÚ¯Ø±ÙˆÙ‡ Ø´Ù‡Ø±ÙŠ: ÙƒÙ„Ù†Ú¯ Ø§...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Ø¬ÙˆØ¯ÙŠ Ø§Ø¨ÙˆØªØŒ Ù†Ø§Ø¨ØºÙ‡ ÙƒØ§ØºØ°ÙŠ\\nØ³Ù…ÙŠÙ‡ Ù†ØµÙŠØ±ÙŠ Ù‡Ø§\\nØ¬ÙˆØ¯ÙŠ Ø§Ø¨...\n",
       "1  Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø±ÙŠØ²ÙŠ Ø¨Ø±Ø§ÙŠ Ù…Ø±Ú¯\\nÙ†Ú¯Ø§Ù‡ÙŠ Ø¨Ù‡ Ø§Ø³ØªØ±Ø³ Ùˆ Ø³Ø§Ø² Ùˆ Ùƒ...\n",
       "2  Ø¢ØºØ§Ø² Ø³Ø§Ø®Øª Ø¨Ø¯Ù†Ù‡ Ùˆ Ø³Ø±Ø±ÙŠØ² Ø¨Ù„Ù†Ø¯ØªØ±ÙŠÙ† Ø³Ø¯\\nÙƒØ´ÙˆØ±\\nØ¹Ù…Ù„ÙŠ...\n",
       "3  Ø¹ÙˆØ§Ù…Ù„ Ù…ÙˆØ«Ø± Ø¯Ø± Ù¾ÙŠØ´Ú¯ÙŠØ±ÙŠ Ø§Ø² Ø¯ÙŠØ§Ø¨Øª\\nØ¯ÙƒØªØ± Ù…Ø­Ù…ÙˆØ¯ Ø¨Ù‡Ø²...\n",
       "4  Ø¢ØºØ§Ø² Ø¹Ù…Ù„ÙŠØ§Øª Ø³Ø§Ø®Øª Ù…ØªØ±Ùˆ ØªØ¬Ø±ÙŠØ´\\nÚ¯Ø±ÙˆÙ‡ Ø´Ù‡Ø±ÙŠ: ÙƒÙ„Ù†Ú¯ Ø§..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hamshahri = pd.DataFrame(data_hamshahri, columns=['text'])\n",
    "data_hamshahri.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5375 entries, 0 to 5374\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5375 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 42.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_hamshahri.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_', 'Ù‹', 'U', '.', 'Ù…', 'Ù', 'Ù', 'Ø´', '\\u200d', 'Ø¡', 'J', '=', 'Øº', 'I', 'Ø­', 'Ú¯', 'S', 'Ø¯', 'b', '\\x7f', '~', 'e', '\\xa0', 'l', 'i', 'y', 'Ùƒ', 'o', 't', 'Ø¬', 'Ø²', 'f', 'Ø«', '+', ';', 'r', 'Y', 'Ù', 'Ø§', '\\n', 'Ù¾', 'Ù‡', 'â€¢', 'Ù‚', 'L', 'Ø›', 'Â¨', 'Ø¢', 'A', 'F', 'p', 'Â½', 'Ø£', '?', 'Ø¤', 'Ùˆ', '2', '\"', 'D', 'Ø¨', '|', ']', 'Â®', '5', 'n', 'ÙŠ', 'Ø©', 'w', 'z', '%', '&', '@', 'c', 'Â¼', '8', 'g', 'Ù‘', 'N', 'Ã§', 'Ú˜', 'q', 'Ø¸', 'â€¦', 'P', '9', 'G', 'k', ',', 'E', 'Q', 'Øª', 'd', 'T', 'Â«', '7', '3', 'Â©', '[', 'Øµ', 'a', 'H', '-', 'Ø¥', '!', 'Ã¢', '/', 'Ø¹', 'j', 'h', 'Ù†', 'Ú†', '0', 'K', ')', 'Ø¦', 'C', 'Ø³', 'Ø°', 'Ø·', 'R', ' ', '(', '\\u200e', 'Ù', 'X', 'Ø®', 'Ù„', 'm', 'u', 'Ø±', 'W', 'v', '1', \"'\", 'M', 'x', 'ØŒ', '4', 'Â»', 'â€“', 'B', 'Ã¨', ':', 'V', 'Ù‰', 'ØŸ', '>', 'Ø¶', 's', '*', '}', 'Ù€', 'Ù', '6'}\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set()\n",
    "for text in data_hamshahri['text']:\n",
    "    unique_chars.update(set(text))\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a CSV file\n",
    "data_tasnim.to_csv('./Datasets/_CSV/tasnim_cleaned.csv', index=False, encoding='utf-8')\n",
    "data_shahnameh.to_csv('./Datasets/_CSV/shahname_cleaned.csv', index=False, encoding='utf-8')\n",
    "data_hamshahri.to_csv('./Datasets/_CSV/hamshahri_cleaned.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class to process the data for the language model\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the DataProcessor object\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            data_path (str): The path to the data file\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.data = []\n",
    "        self.cleaned_data = []\n",
    "        self.tokenized_data = []\n",
    "        self.prepared_data = []\n",
    "\n",
    "        self.read_data()\n",
    "        self.clean_text()\n",
    "        self.tokenizer()\n",
    "        frequencies_dict, n_tokens, n_unique_tokens = self.get_most_freqs()\n",
    "        print(f\"Number of tokens: {n_tokens}\")\n",
    "        print(f\"Number of unique tokens: {n_unique_tokens}\")\n",
    "        self.handle_unknown()\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"\n",
    "        Read the data from the file\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(self.data_path, encoding='utf-8')['text'].dropna().tolist()\n",
    "    \n",
    "    def clean_text(self):\n",
    "        cleaned_data = [re.sub(r'[^Ø¢-ÛŒÛ°-Û¹.ØŸ\\s]', '', line) for line in self.data]   # Remove non-persian characters\n",
    "        cleaned_data = [re.sub(r'[Û°-Û¹]+', 'N', line) for line in cleaned_data]      # Replace numbers with 'N'\n",
    "        cleaned_data = [re.sub(r'\\s+', ' ', line) for line in cleaned_data]         # Remove extra spaces\n",
    "        cleaned_data = [line.strip() for line in cleaned_data]                      # Remove leading and trailing spaces\n",
    "        self.cleaned_data = cleaned_data\n",
    "    \n",
    "    def tokenizer(self):\n",
    "        \"\"\"\n",
    "        Tokenize the cleaned data\n",
    "        \"\"\"\n",
    "        self.tokenized_data = [line.split() for line in self.cleaned_data]\n",
    "    \n",
    "    def get_most_freqs(self, top_n=10000):\n",
    "        \"\"\"\n",
    "        Get the most frequent tokens in the data\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            top_n (int): The number of most frequent tokens to keep\n",
    "        Returns:\n",
    "            frequencies_dict (dict): A dictionary of the most frequent tokens and their counts\n",
    "            n_tokens (int): The total number of tokens in the data\n",
    "            n_unique_tokens (int): The number of unique tokens in the data\n",
    "        \"\"\"\n",
    "        all_tokens = [token for sentence in self.tokenized_data for token in sentence]\n",
    "        frequencies = Counter(all_tokens)\n",
    "        most_freq = frequencies.most_common(top_n)\n",
    "        self.frequencies_dict = dict(most_freq)\n",
    "        with open('frequent.txt', 'w', encoding='utf-8') as f:\n",
    "            f.writelines([f\"{token}\\t{count}\\n\" for token, count in most_freq[:200]])\n",
    "        self.n_tokens = len(all_tokens)\n",
    "        self.n_unique_tokens = len(frequencies.keys())\n",
    "        return self.frequencies_dict, self.n_tokens, self.n_unique_tokens\n",
    "    \n",
    "    def handle_unknown(self):\n",
    "        \"\"\"\n",
    "        Replace the less frequent tokens with '<UNK>'\n",
    "        \"\"\"\n",
    "        self.prepared_data = [[word if word in self.frequencies_dict else \"<UNK>\" for word in sentence] for sentence in self.tokenized_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    \"\"\"\n",
    "    A class to build an N-gram language model\n",
    "    \"\"\"\n",
    "    def __init__(self, prepared_data):\n",
    "        \"\"\"\n",
    "        Initialize the NgramLanguageModel object\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            prepared_data (list): The preprocessed data\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.prepared_data = prepared_data\n",
    "        self.unigram_dict = Counter()\n",
    "        self.bigram_dict = defaultdict(Counter)\n",
    "        self.trigram_dict = defaultdict(Counter)\n",
    "\n",
    "        self.make_ngram_dicts()\n",
    "        print(f\"Number of unigrams: {len(self.unigram_dict)}\")\n",
    "        print(f\"Number of bigrams: {len(self.bigram_dict)}\")\n",
    "        print(f\"Number of trigrams: {len(self.trigram_dict)}\")\n",
    "    \n",
    "    def make_ngram_dicts(self):\n",
    "        \"\"\"\n",
    "        Create dictionaries for unigrams, bigrams, and trigrams\n",
    "        \"\"\"\n",
    "        for sentence in self.prepared_data:\n",
    "            for i in range(len(sentence)):\n",
    "                self.unigram_dict[sentence[i]] += 1\n",
    "                if i < len(sentence) - 1:    # if i > 0:\n",
    "                    self.bigram_dict[sentence[i-1]][sentence[i]] += 1\n",
    "                if i < len(sentence) - 2:    # if i > 1:\n",
    "                    self.trigram_dict[(sentence[i-2], sentence[i-1])][sentence[i]] += 1\n",
    "    \n",
    "    def calculate_smoothed_probs(self, n, input_text):\n",
    "        \"\"\"\n",
    "        Calculate the smoothed probabilities of the next word\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            n (int): The n-gram model to use\n",
    "            input_text (str): The input text\n",
    "        Returns:\n",
    "            probs (dict): A dictionary of the probabilities of the next word\n",
    "        \"\"\"\n",
    "        words = input_text.split()\n",
    "        probs = {}\n",
    "\n",
    "        for candidate in self.unigram_dict.keys():\n",
    "            word = candidate\n",
    "            if n == 2:\n",
    "                prev_word = words[-1]\n",
    "                probs[word] = (self.bigram_dict[prev_word][word] + 1 / (self.unigram_dict[prev_word] + len(self.unigram_dict)))\n",
    "            elif n == 3:\n",
    "                prev_bigram = (words[-2], words[-1])\n",
    "                probs[word] = (self.trigram_dict[prev_bigram][word] + 1 / (self.bigram_dict[prev_bigram[1]][word] + len(self.bigram_dict)))\n",
    "\n",
    "        return probs\n",
    "    \n",
    "    def generate_text(self, n, input_text=None, max_length=20):\n",
    "        \"\"\"\n",
    "        Generate text using the n-gram model\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            n (int): The n-gram model to use\n",
    "            input_text (str): The input text\n",
    "            max_length (int): The maximum length of the generated text\n",
    "        Returns:\n",
    "            text (str): The generated text\n",
    "        \"\"\"\n",
    "        if not input_text:\n",
    "            input_text = \" \".join(np.random.choice(list(self.unigram_dict.keys()), 3))\n",
    "            \n",
    "        text = input_text.split()\n",
    "        \n",
    "        print(input_text, end=' ')\n",
    "        for _ in range(max_length):\n",
    "            time.sleep(0.01)\n",
    "            probs = self.calculate_smoothed_probs(n, \" \".join(text))\n",
    "            # probs = self.kneser_ney_smoothing(2, \" \".join(text))\n",
    "            if not probs:\n",
    "                break\n",
    "            next_word = max(probs, key=probs.get)\n",
    "            text.append(next_word)\n",
    "            print(next_word, end=' ')\n",
    "        \n",
    "        return \" \".join(text)\n",
    "    \n",
    "    def evaluate_model(self, prepared_test_data, n):\n",
    "        \"\"\"\n",
    "        Evaluate the model using the test data\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            prepared_test_data (list): The preprocessed test data\n",
    "            n (int): The n-gram model to use\n",
    "        Returns:\n",
    "            log_likelihood (float): The log-likelihood of the model\n",
    "            perplexity (float): The perplexity of the model\n",
    "        \"\"\"\n",
    "        log_likelihood = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for sentence in prepared_test_data[:300]:\n",
    "            if len(sentence) < 3:\n",
    "                continue\n",
    "            input_text = \" \".join(sentence)\n",
    "            probs = self.calculate_smoothed_probs(n, input_text)\n",
    "            \n",
    "            for p in probs.values():\n",
    "                log_likelihood += np.log(p)\n",
    "                total_tokens += 1\n",
    "        \n",
    "        avg_log_likelihood = log_likelihood / total_tokens if total_tokens > 0 else float('-inf')\n",
    "        perplexity = np.exp(-avg_log_likelihood)\n",
    "        return avg_log_likelihood, perplexity\n",
    "    \n",
    "    def kneser_ney_smoothing(self, n, input_text, d=0.75):\n",
    "        \"\"\"\n",
    "        Calculate the Kneser-Ney smoothed probabilities of the next word\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            n (int): The n-gram model to use\n",
    "            input_text (str): The input text\n",
    "            d (float): The discount factor\n",
    "        Returns:\n",
    "            probs (list): A list of the probabilities of the next word\n",
    "        \"\"\"\n",
    "        words = input_text.split()\n",
    "        probs = {}\n",
    "\n",
    "        for candidate in self.unigram_dict.keys():\n",
    "            if candidate in words:\n",
    "                continue\n",
    "            word = candidate\n",
    "            context = \" \".join(words[-n+1:])\n",
    "            context_words = context.split()\n",
    "            context_word_counts = {}\n",
    "            for i in range(1, n):\n",
    "                context_word_counts[context_words[-i]] = len(self.bigram_dict[context_words[-i]])\n",
    "            if n == 2:\n",
    "                probs[word] = max(self.bigram_dict[context_words[-1]][word] - d, 0) / self.unigram_dict[context_words[-1]] + d * len(self.bigram_dict[context_words[-1]]) / self.unigram_dict[context_words[-1]] * self.bigram_dict[context_words[-1]][word]\n",
    "            else:\n",
    "                probs[word] = 0\n",
    "        \n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 567159\n",
      "Number of unique tokens: 18944\n"
     ]
    }
   ],
   "source": [
    "# tasnim_processor = DataProcessor('./Datasets/_CSV/tasnim_cleaned.csv')\n",
    "shahname_processor = DataProcessor('./Datasets/_CSV/shahname_cleaned.csv')\n",
    "# hamshahri_processor = DataProcessor('./Datasets/_CSV/hamshahri_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepapred_data = shahname_processor.prepared_data\n",
    "train_size = int(0.95 * len(prepapred_data))\n",
    "train_data = prepapred_data[:train_size]\n",
    "test_data = prepapred_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams: 9984\n",
      "Number of bigrams: 9700\n",
      "Number of trigrams: 143636\n"
     ]
    }
   ],
   "source": [
    "lm = NgramLanguageModel(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Ø¨Ù‡ ÛŒØ§Ø¯ Ù†Ø¯Ø§Ø±Ø¯ Ø¬Ù‡Ø§Ù† Ú†ÙˆÙ† Ø´Ø¨ ØªÛŒØ±Ù‡ \n",
      "Ø´Ø¨ ØªÛŒØ±Ù‡ Ø´Ø¯ Ø¢Ù† Ù†Ø§Ù…ÙˆØ± Ú¯Ø°Ø± Ú©Ø±Ø¯ \n",
      "Ú¯Ø°Ø± Ú©Ø±Ø¯ Ø¨Ø± Ø®ÙˆÛŒØ´ØªÙ† ØªØ§Ø± Ø¨Ù‡ Ø®Ø¯Ø§ÙˆÙ†Ø¯ \n",
      "Ø¨Ù‡ Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¨Ù‡ Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¨Ù‡ Ø®Ø¯Ø§ÙˆÙ†Ø¯ Ø¨Ù‡ \n"
     ]
    }
   ],
   "source": [
    "input_text = \"Ø¨Ù‡ ÛŒØ§Ø¯\"\n",
    "print(\"Generated text:\")\n",
    "# generated_text = lm.generate_text(3, input_text=None, max_length=20)\n",
    "generated_text = lm.generate_text(3, input_text, max_length=5)\n",
    "print()\n",
    "for _ in range(3):\n",
    "    generated_text = lm.generate_text(3, input_text=(generated_text.split()[-2] + ' ' + generated_text.split()[-1]), max_length=5)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model (n=3):\n",
      "Log likelihood: -9.170771065368235\n",
      "Perplexity: 9612.03333939397\n",
      "Evaluating the model (n=2):\n",
      "Log likelihood: -9.06635798044812\n",
      "Perplexity: 8659.029951535555\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the model (n=3):\")\n",
    "log_likelihood, perplexity = lm.evaluate_model(test_data, 3)\n",
    "print(f\"Log likelihood: {log_likelihood}\")\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print()\n",
    "print(\"Evaluating the model (n=2):\")\n",
    "log_likelihood, perplexity = lm.evaluate_model(test_data, 2)\n",
    "print(f\"Log likelihood: {log_likelihood}\")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kneser-Ney Smoothing:\n",
      "Ù†Ø²Ø¯ÛŒÚ©: 27.301329740495575\n",
      "Ù¾ÛŒØ´: 23.0166071114756\n",
      "Ù‡Ø±: 18.237493409876397\n",
      "Ú©Ø±Ø¯Ø§Ø±: 15.271146974401029\n",
      "Ø³Ø±: 13.89783843945873\n",
      "Add-1 Smoothing:\n",
      "Ú†Ø§Ø±Ù‡: 1.0001029442042413\n",
      "Ø§Ø²: 1.0001026377912348\n",
      "Ø¯Ø±Ú¯Ø§Ù‡: 1.0001020199959192\n",
      "Ø¬Ù†Ú¯: 1.0001017501017502\n",
      "Ø§ÛŒØ±Ø§Ù†: 1.000100969305331\n"
     ]
    }
   ],
   "source": [
    "# Kneser-Ney Smoothing\n",
    "print(\"Kneser-Ney Smoothing:\")\n",
    "input_text = \"Ú†Ùˆ Ø±Ø³ØªÙ… Ø¨ÛŒØ§ÛŒØ¯ Ø¨Ù‡\"\n",
    "probs = lm.kneser_ney_smoothing(2, input_text)\n",
    "\n",
    "# Print top 5 probabilities (sorted by value)\n",
    "sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, prob in sorted_probs[:5]:\n",
    "    print(f\"{word}: {prob}\")\n",
    "\n",
    "# Smoothing\n",
    "print(\"Add-1 Smoothing:\")\n",
    "probs = lm.calculate_smoothed_probs(3, input_text)\n",
    "\n",
    "# Print top 5 probabilities (sorted by value)\n",
    "sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, prob in sorted_probs[:5]:\n",
    "    print(f\"{word}: {prob}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
