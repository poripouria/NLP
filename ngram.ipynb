{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "    Project 1: N-Grams Language Model\n",
    "\n",
    "    Project for Amirkabir University of Technilogy (Tehran Polytechnic), Computer Scince department\n",
    "    Natural Language Processing course\n",
    "\n",
    "Student Name & ID: Pouria Alimoradpor 403112088\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = {\n",
    "    'Tasnim': './Datasets/Tasnim/tasnim.csv',\n",
    "    'Shahnameh': './Datasets/Shahnameh/shahname.csv',\n",
    "    'Hamshahri': './Datasets/HamshahriData/HamshahriCorpus'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data: Tasnim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>سیاسی</td>\n",
       "      <td>میرکاظمی: زمان حذف ارز ۴۲۰۰ تومانی مشخص نیست</td>\n",
       "      <td>رئیس سازمان برنامه و بودجه گفت: هر زمان شرایط...</td>\n",
       "      <td>به گزارش گروه پارلمانی  ، «مسعود میرکاظمی» رئی...</td>\n",
       "      <td>۲۴ فروردين ۱۴۰۱ - ۰۹:۵۰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>سیاسی</td>\n",
       "      <td>طرح ۲ فوریتی شفافیت قوای سه‌گانه با ۲۰۰ امضا ...</td>\n",
       "      <td>نماینده نیشابور در مجلس از ارائه طرح ۲ فوریتی...</td>\n",
       "      <td>احسان ارکانی نماینده مردم نیشابور در مجلس شورا...</td>\n",
       "      <td>۲۴ فروردين ۱۴۰۱ - ۰۹:۵۰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>سیاسی</td>\n",
       "      <td>رئیسی انتخاب نخست وزیر جدید پاکستان را تبریک ...</td>\n",
       "      <td>رئیس جمهور کشورمان طی پیامی انتخاب نخست وزیر ...</td>\n",
       "      <td>به گزارش حوزه دولت  ، آیت‌الله سید ابراهیم رئی...</td>\n",
       "      <td>۲۵ فروردين ۱۴۰۱ - ۱۳:۵۴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>سیاسی</td>\n",
       "      <td>اصلاح اساسنامه شرکت شهر فرودگاهی امام خمینی (...</td>\n",
       "      <td>اساسنامه شرکت شهر فرودگاهی امام خمینی(ره) در ...</td>\n",
       "      <td>به گزارش حوزه دولت  ، در جلسه صبح روز چهارشنبه...</td>\n",
       "      <td>۳۱ فروردين ۱۴۰۱ - ۱۲:۰۳</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>سیاسی</td>\n",
       "      <td>ارائه طرحی کلی و مبهم برای شفافیت فرار از مطا...</td>\n",
       "      <td>نماینده مردم تهران گفت: در شرایطی که طرح شفاف...</td>\n",
       "      <td>علی خضریان نماینده تهران در مجلس در گفت‌وگو با...</td>\n",
       "      <td>۳۰ فروردين ۱۴۰۱ - ۱۶:۱۵</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                              title  \\\n",
       "0    سیاسی      میرکاظمی: زمان حذف ارز ۴۲۰۰ تومانی مشخص نیست    \n",
       "1    سیاسی   طرح ۲ فوریتی شفافیت قوای سه‌گانه با ۲۰۰ امضا ...   \n",
       "2    سیاسی   رئیسی انتخاب نخست وزیر جدید پاکستان را تبریک ...   \n",
       "3    سیاسی   اصلاح اساسنامه شرکت شهر فرودگاهی امام خمینی (...   \n",
       "4    سیاسی   ارائه طرحی کلی و مبهم برای شفافیت فرار از مطا...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0   رئیس سازمان برنامه و بودجه گفت: هر زمان شرایط...   \n",
       "1   نماینده نیشابور در مجلس از ارائه طرح ۲ فوریتی...   \n",
       "2   رئیس جمهور کشورمان طی پیامی انتخاب نخست وزیر ...   \n",
       "3   اساسنامه شرکت شهر فرودگاهی امام خمینی(ره) در ...   \n",
       "4   نماینده مردم تهران گفت: در شرایطی که طرح شفاف...   \n",
       "\n",
       "                                                body  \\\n",
       "0  به گزارش گروه پارلمانی  ، «مسعود میرکاظمی» رئی...   \n",
       "1  احسان ارکانی نماینده مردم نیشابور در مجلس شورا...   \n",
       "2  به گزارش حوزه دولت  ، آیت‌الله سید ابراهیم رئی...   \n",
       "3  به گزارش حوزه دولت  ، در جلسه صبح روز چهارشنبه...   \n",
       "4  علی خضریان نماینده تهران در مجلس در گفت‌وگو با...   \n",
       "\n",
       "                         time  \n",
       "0    ۲۴ فروردين ۱۴۰۱ - ۰۹:۵۰   \n",
       "1    ۲۴ فروردين ۱۴۰۱ - ۰۹:۵۰   \n",
       "2    ۲۵ فروردين ۱۴۰۱ - ۱۳:۵۴   \n",
       "3    ۳۱ فروردين ۱۴۰۱ - ۱۲:۰۳   \n",
       "4    ۳۰ فروردين ۱۴۰۱ - ۱۶:۱۵   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tasnim = pd.read_csv(data_path['Tasnim'], encoding='utf-8')\n",
    "data_tasnim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63494 entries, 0 to 63493\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  63494 non-null  object\n",
      " 1   title     63493 non-null  object\n",
      " 2   abstract  63493 non-null  object\n",
      " 3   body      62558 non-null  object\n",
      " 4   time      63493 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data_tasnim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>میرکاظمی: زمان حذف ارز ۴۲۰۰ تومانی مشخص نیست ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طرح ۲ فوریتی شفافیت قوای سه‌گانه با ۲۰۰ امضا ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>رئیسی انتخاب نخست وزیر جدید پاکستان را تبریک ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اصلاح اساسنامه شرکت شهر فرودگاهی امام خمینی (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ارائه طرحی کلی و مبهم برای شفافیت فرار از مطا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0   میرکاظمی: زمان حذف ارز ۴۲۰۰ تومانی مشخص نیست ...\n",
       "1   طرح ۲ فوریتی شفافیت قوای سه‌گانه با ۲۰۰ امضا ...\n",
       "2   رئیسی انتخاب نخست وزیر جدید پاکستان را تبریک ...\n",
       "3   اصلاح اساسنامه شرکت شهر فرودگاهی امام خمینی (...\n",
       "4   ارائه طرحی کلی و مبهم برای شفافیت فرار از مطا..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tasnim = data_tasnim.drop(columns=['category', 'time'])\n",
    "data_tasnim['text'] = data_tasnim['title'] + '. ' + data_tasnim['abstract'] + ' ' + data_tasnim['body']\n",
    "data_tasnim = data_tasnim.drop(columns=['title', 'abstract', 'body'])\n",
    "data_tasnim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " میرکاظمی: زمان حذف ارز ۴۲۰۰ تومانی مشخص نیست .  رئیس سازمان برنامه و بودجه گفت: هر زمان شرایط و فضای اقتصادی کشور مساعد باشد، می‌توان به تدریج حذف ارز ترجیحی را عملیاتی کرد و در حال حاضر برای حذف ارز ۴۲۰۰ تومانی زمان و تاریخ مشخصی را نمی‌توان تعیین کرد.  به گزارش گروه پارلمانی  ، «مسعود میرکاظمی» رئیس سازمان برنامه و بودجه، در پاسخ به سؤالی مبنی بر اینکه دولت از چه زمانی اقدام به حذف ارز 4200 تومانی خواهد کرد، گفت: براساس قانون بودجه سال 1401، اختیار حذف و یا عدم حذف ارز ترجیحی برعهده دولت قرار داده شده است و از سوی دیگر حجم ارز 4200 تومانی در کشور به میزانی نیست که بتوان تا پایان سال از آن استفاده کرد. وی که با خانه ملت گفت‌وگو کرده است ادامه داد: بنابراین هر زمان شرایط و فضای اقتصادی کشور مساعد باشد، می‌توان به تدریج این موضوع را عملیاتی کرد و در حال حاضر برای حذف ارز ترجیحی زمان و تاریخ مشخصی را نمی‌توان تعیین کرد. رئیس سازمان برنامه و بودجه در پاسخ به سؤال دیگری مبنی بر اینکه چرا با توجه به اینکه هنوز ارز ترجیحی حذف نشده است، شاهد افزایش قیمت کالاها به ویژه در مورد گوشت قرمز و مرغ در کشور هستیم، اظهار کرد: قیمت‌ها روی نرخ گوشت قرمز و مرغ تغییری نکرده است؛ برخی هزینه‌ها و قیمت تمام شده مواد غذایی به نرخ مواد اولیه بستگی ندارد و عوامل دیگری نیز در این میان مؤثر هستند و برای این تأثیرات باید یارانه پرداخت شود و یا اجازه داد قیمت‌ها اصلاح شود. انتهای پیام/؛\n"
     ]
    }
   ],
   "source": [
    "print(data_tasnim['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ً', '２', '✌', 'Ş', '\\u2003', '４', '۸', 'ﮒ', 'ﺾ', 'ύ', 'ۀ', '\\u200d', '=', 'J', 'р', 'б', 'I', 'ø', 'ۖ', 'к', 'ﻈ', 'ﮕ', 'É', 'ﺵ', '５', 'С', 'ھ', 'ú', 'د', 'ﮔ', 'ﻄ', '\\u202c', 'ﭼ', 'ı', 'ﺶ', 'ٔ', '🔸', 'ﮐ', 'γ', '^', '۵', 'β', 'e', 'í', 'i', 'ﻚ', 'y', '−', 'в', 't', 'ü', 'O', 'ز', 'ۗ', 'ﻣ', 'ﻌ', 'е', 'ﺗ', 'f', 'а', '+', '📱', 'ۙ', 'r', 'υ', '\\u2069', '≈', 'َ', '\\u200a', 'ﺻ', '﴾', '#', '”', 'κ', 'ق', 'ﮏ', 'ﺘ', 'ڼ', '⁵', 'آ', '\\u2028', '۰', 'A', 'ó', '۳', 'p', 'ο', 'ﻢ', '٫', '٩', 'Ε', '👈', 'ٴ', '2', 'ب', 'ﻤ', '|', '\\uf0d8', 'Æ', 'n', 'ﺤ', '`', 'ι', 'ة', 'w', 'ﻓ', 'ﻕ', 'z', '\\x88', 'c', 'ﺝ', 'ы', 'ڋ', '\\u202b', 'g', 'ﻛ', 'ﻭ', 'ّ', 'ﻻ', 'ﮓ', 'ﻍ', 'ﻆ', 'ﻨ', 'é', 'ğ', 'ﻱ', 'N', 'Ρ', 'q', '…', 'ﯾ', '9', 'G', '‼', '‹', 'ﭽ', 'ﭻ', 'ﻐ', 'ﻼ', '\\u200b', 'ﻑ', 'Q', 'ت', '６', 'ﻯ', 'ﻘ', 'ٕ', 'ν', 'd', 'õ', 'ﮑ', '🔴', 'ﺕ', '3', 'ﻰ', '[', 'ﺌ', '⦁', '۱', 'ﺨ', 'ﺏ', 'á', 'ﺣ', 'ﻊ', 'Á', 'ە', '🌹', 'α', '؍', 'ٌ', '!', 'ﻒ', '۔', 'ﻳ', 'ﻉ', '/', 'ﻇ', 'ﺢ', 'ﺲ', 'ع', 'ﻟ', 'ﻸ', 'ﷲ', '😒', '☑', 'ﺼ', 'ﺐ', '٢', '\\x9d', 'ﻡ', 'R', '(', '\\u200e', 'خ', 'л', 'ﺜ', '\\xad', 'u', 'm', 'ﺰ', '🔷', 'W', 'ř', 'σ', '×', '“', 'ں', 'ω', 'v', 'ﻖ', \"'\", 'M', '😉', 'x', '4', '»', '–', 'B', ':', '🔺', 'V', 'ى', '؟', '>', '\\u2005', 'ﻜ', 'ض', 'ﺡ', 'ﭘ', '*', 'ﻞ', 'τ', '⛔', '🔽', 'ﮎ', 'Т', '۲', '6', '_', 'ﻦ', '🔰', 'ﻫ', 'څ', 'U', 'ﻴ', 'Δ', '.', 'م', 'ٍ', '🏼', 'ف', '️', '—', 'ﺭ', 'ﺧ', 'ش', 'н', 'δ', 'ﭙ', 'ء', 'غ', 'ۚ', 'ﺹ', '\\u202a', '\\x83', '７', '✍', 'ح', 'گ', 'S', '±', 'ٱ', 'ﺄ', 'b', '🔹', 'ٖ', '\\u061c', '●', '¤', '\\u2066', '💢', 'ﻧ', '٤', 'ٲ', 'İ', '👉', 'ﺪ', 'ﺳ', '~', '\\xa0', 'ٹ', 'l', 'и', 'ك', 'ﺿ', 'o', 'ﺋ', '３', '１', '۶', 'ج', 'ö', '\\u2063', 'ε', '’', 'ﺍ', '▫', '¡', '\\u200f', 'ث', 'К', ';', 'ﺮ', 'Y', 'ﺺ', 'П', 'ا', 'پ', '٧', 'ه', '•', 'ﻂ', 'с', 'ی', 'L', 'ﺑ', '؛', 'π', 'ﻩ', 'ﺯ', 'F', '١', 'أ', 'ﻃ', '?', 'ؤ', 'й', '٥', 'و', 'ٓ', '٦', 'ô', 'ñ', 'ﺩ', '\"', 'D', '\\ufeff', '📷', ']', '5', 'ﺱ', 'о', 'ي', '‘', 'ْ', '&', '%', '@', 'ﺞ', '8', 'ﻲ', '¬', '》', 'Б', 'ă', 'μ', 'ί', 'ژ', 'ې', 'ç', 'ظ', '{', 'P', '$', 'k', 'ć', '٬', ',', 'ﻏ', 'д', 'î', 'ﺟ', 'Ö', 'E', 'т', 'ٰ', 'ﺴ', 'ﺷ', '٣', 'T', '«', 'ﮋ', '7', '۹', '<', '📸', 'г', 'ص', 'H', 'a', '😢', '🛑', '-', 'п', 'إ', 'ﻔ', '�', 'ﯼ', 'ﯿ', 'ﺎ', 'ﺒ', '›', 'j', 'ﯽ', 'h', 'ﻋ', '\\u202d', 'ﺁ', 'ﺫ', '٪', 'ن', 'چ', '0', 'K', '۴', 'ã', ')', 'ئ', 'ﻝ', 'ﺖ', 'C', 'ā', '\\u2009', 'س', 'ذ', '٨', 'ط', '《', 'Ü', ' ', 'ُ', 'ﻪ', 'X', 'у', 'ē', 'ﺸ', 'ل', '\\x96', 'ﻎ', 'ﻁ', '÷', '🔻', 'ر', 'м', 'θ', '\\u200c', 'š', '1', '،', 'Z', 'ț', 'ﻬ', 'ﻀ', '۷', 'ک', 's', '\\ufff8', '\\\\', 'ş', '·', '}', 'ـ', 'ﻮ', '\\u2067', 'ﺛ', 'ﻥ', 'ﺬ', 'ﻗ', '♦', '﴿', 'ِ', 'ä'}\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set()\n",
    "for text in data_tasnim['text'].dropna():\n",
    "    unique_chars.update(set(text))\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data: Shahnameh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Part</th>\n",
       "      <th>Bait</th>\n",
       "      <th>Mesra</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>به نام خداوند جان و خرد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>کز این برتر اندیشه بر نگذرد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>خداوند نام و خداوند جای</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>خداوند روزی ده رهنمای</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>خداوند کیوان و گَردان سپهر</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chapter  Part  Bait  Mesra                         Text\n",
       "0        1     1     1      1      به نام خداوند جان و خرد\n",
       "1        1     1     1      2  کز این برتر اندیشه بر نگذرد\n",
       "2        1     1     2      1      خداوند نام و خداوند جای\n",
       "3        1     1     2      2        خداوند روزی ده رهنمای\n",
       "4        1     1     3      1   خداوند کیوان و گَردان سپهر"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shahnameh = pd.read_csv(data_path['Shahnameh'], encoding='utf-8')\n",
    "data_shahnameh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99222 entries, 0 to 99221\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Chapter  99222 non-null  int64 \n",
      " 1   Part     99222 non-null  int64 \n",
      " 2   Bait     99222 non-null  int64 \n",
      " 3   Mesra    99222 non-null  int64 \n",
      " 4   Text     99222 non-null  object\n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data_shahnameh.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>به نام خداوند جان و خرد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>کز این برتر اندیشه بر نگذرد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>خداوند نام و خداوند جای</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>خداوند روزی ده رهنمای</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>خداوند کیوان و گَردان سپهر</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text\n",
       "0      به نام خداوند جان و خرد\n",
       "1  کز این برتر اندیشه بر نگذرد\n",
       "2      خداوند نام و خداوند جای\n",
       "3        خداوند روزی ده رهنمای\n",
       "4   خداوند کیوان و گَردان سپهر"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shahnameh = data_shahnameh.drop(columns=['Chapter', 'Part', 'Bait', 'Mesra'])\n",
    "data_shahnameh = data_shahnameh.rename(columns={'Text': 'text'})\n",
    "data_shahnameh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ع', 'ج', 'ز', 'ّ', 'م', 'ن', 'ف', 'چ', 'ث', ')', 'ئ', 'ژ', 'ش', 'ذ', 'س', 'ط', 'ظ', ' ', 'ء', '(', 'َ', 'غ', 'ا', 'ُ', 'خ', 'پ', 'ه', 'ل', 'ح', 'ق', 'ی', '؛', 'گ', 'ت', 'ر', 'آ', '\\u200c', 'د', '«', 'ٔ', '،', '»', 'ص', 'أ', ':', 'ؤ', 'و', '؟', 'ک', 'ض', 'ب', '\\xa0', 'ي', '!', 'ْ', 'ِ'}\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set()\n",
    "for text in data_shahnameh['text']:\n",
    "    unique_chars.update(set(text))\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data: Hamshahri**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hamshahri = []\n",
    "for year_folder in os.listdir(data_path['Hamshahri']):\n",
    "    folder_path = os.path.join(data_path['Hamshahri'], year_folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".ham\"):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data_hamshahri.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>جودي ابوت، نابغه كاغذي\\nسميه نصيري ها\\nجودي اب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>برنامه ريزي براي مرگ\\nنگاهي به استرس و ساز و ك...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>آغاز ساخت بدنه و سرريز بلندترين سد\\nكشور\\nعملي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>عوامل موثر در پيشگيري از ديابت\\nدكتر محمود بهز...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آغاز عمليات ساخت مترو تجريش\\nگروه شهري: كلنگ ا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  جودي ابوت، نابغه كاغذي\\nسميه نصيري ها\\nجودي اب...\n",
       "1  برنامه ريزي براي مرگ\\nنگاهي به استرس و ساز و ك...\n",
       "2  آغاز ساخت بدنه و سرريز بلندترين سد\\nكشور\\nعملي...\n",
       "3  عوامل موثر در پيشگيري از ديابت\\nدكتر محمود بهز...\n",
       "4  آغاز عمليات ساخت مترو تجريش\\nگروه شهري: كلنگ ا..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hamshahri = pd.DataFrame(data_hamshahri, columns=['text'])\n",
    "data_hamshahri.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5375 entries, 0 to 5374\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5375 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 42.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_hamshahri.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_', 'ً', 'U', '.', 'م', 'ٍ', 'ف', 'ش', '\\u200d', 'ء', 'J', '=', 'غ', 'I', 'ح', 'گ', 'S', 'د', 'b', '\\x7f', '~', 'e', '\\xa0', 'l', 'i', 'y', 'ك', 'o', 't', 'ج', 'ز', 'f', 'ث', '+', ';', 'r', 'Y', 'َ', 'ا', '\\n', 'پ', 'ه', '•', 'ق', 'L', '؛', '¨', 'آ', 'A', 'F', 'p', '½', 'أ', '?', 'ؤ', 'و', '2', '\"', 'D', 'ب', '|', ']', '®', '5', 'n', 'ي', 'ة', 'w', 'z', '%', '&', '@', 'c', '¼', '8', 'g', 'ّ', 'N', 'ç', 'ژ', 'q', 'ظ', '…', 'P', '9', 'G', 'k', ',', 'E', 'Q', 'ت', 'd', 'T', '«', '7', '3', '©', '[', 'ص', 'a', 'H', '-', 'إ', '!', 'â', '/', 'ع', 'j', 'h', 'ن', 'چ', '0', 'K', ')', 'ئ', 'C', 'س', 'ذ', 'ط', 'R', ' ', '(', '\\u200e', 'ُ', 'X', 'خ', 'ل', 'm', 'u', 'ر', 'W', 'v', '1', \"'\", 'M', 'x', '،', '4', '»', '–', 'B', 'è', ':', 'V', 'ى', '؟', '>', 'ض', 's', '*', '}', 'ـ', 'ِ', '6'}\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set()\n",
    "for text in data_hamshahri['text']:\n",
    "    unique_chars.update(set(text))\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a CSV file\n",
    "data_tasnim.to_csv('./Datasets/_CSV/tasnim_cleaned.csv', index=False, encoding='utf-8')\n",
    "data_shahnameh.to_csv('./Datasets/_CSV/shahname_cleaned.csv', index=False, encoding='utf-8')\n",
    "data_hamshahri.to_csv('./Datasets/_CSV/hamshahri_cleaned.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class to process the data for the language model\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the DataProcessor object\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            data_path (str): The path to the data file\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.data = []\n",
    "        self.cleaned_data = []\n",
    "        self.tokenized_data = []\n",
    "        self.prepared_data = []\n",
    "\n",
    "        self.read_data()\n",
    "        self.clean_text()\n",
    "        self.tokenizer()\n",
    "        frequencies_dict, n_tokens, n_unique_tokens = self.get_most_freqs()\n",
    "        print(f\"Number of tokens: {n_tokens}\")\n",
    "        print(f\"Number of unique tokens: {n_unique_tokens}\")\n",
    "        self.handle_unknown()\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"\n",
    "        Read the data from the file\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(self.data_path, encoding='utf-8')['text'].dropna().tolist()\n",
    "    \n",
    "    def clean_text(self):\n",
    "        cleaned_data = [re.sub(r'[^آ-ی۰-۹.؟\\s]', '', line) for line in self.data]   # Remove non-persian characters\n",
    "        cleaned_data = [re.sub(r'[۰-۹]+', 'N', line) for line in cleaned_data]      # Replace numbers with 'N'\n",
    "        cleaned_data = [re.sub(r'\\s+', ' ', line) for line in cleaned_data]         # Remove extra spaces\n",
    "        cleaned_data = [line.strip() for line in cleaned_data]                      # Remove leading and trailing spaces\n",
    "        self.cleaned_data = cleaned_data\n",
    "    \n",
    "    def tokenizer(self):\n",
    "        \"\"\"\n",
    "        Tokenize the cleaned data\n",
    "        \"\"\"\n",
    "        self.tokenized_data = [line.split() for line in self.cleaned_data]\n",
    "    \n",
    "    def get_most_freqs(self, top_n=10000):\n",
    "        \"\"\"\n",
    "        Get the most frequent tokens in the data\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            top_n (int): The number of most frequent tokens to keep\n",
    "        Returns:\n",
    "            frequencies_dict (dict): A dictionary of the most frequent tokens and their counts\n",
    "            n_tokens (int): The total number of tokens in the data\n",
    "            n_unique_tokens (int): The number of unique tokens in the data\n",
    "        \"\"\"\n",
    "        all_tokens = [token for sentence in self.tokenized_data for token in sentence]\n",
    "        frequencies = Counter(all_tokens)\n",
    "        most_freq = frequencies.most_common(top_n)\n",
    "        self.frequencies_dict = dict(most_freq)\n",
    "        with open('frequent.txt', 'w', encoding='utf-8') as f:\n",
    "            f.writelines([f\"{token}\\t{count}\\n\" for token, count in most_freq[:200]])\n",
    "        self.n_tokens = len(all_tokens)\n",
    "        self.n_unique_tokens = len(frequencies.keys())\n",
    "        return self.frequencies_dict, self.n_tokens, self.n_unique_tokens\n",
    "    \n",
    "    def handle_unknown(self):\n",
    "        \"\"\"\n",
    "        Replace the less frequent tokens with '<UNK>'\n",
    "        \"\"\"\n",
    "        self.prepared_data = [[word if word in self.frequencies_dict else \"<UNK>\" for word in sentence] for sentence in self.tokenized_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    \"\"\"\n",
    "    A class to build an N-gram language model\n",
    "    \"\"\"\n",
    "    def __init__(self, prepared_data):\n",
    "        \"\"\"\n",
    "        Initialize the NgramLanguageModel object\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            prepared_data (list): The preprocessed data\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.prepared_data = prepared_data\n",
    "        self.unigram_dict = Counter()\n",
    "        self.bigram_dict = defaultdict(Counter)\n",
    "        self.trigram_dict = defaultdict(Counter)\n",
    "\n",
    "        self.make_ngram_dicts()\n",
    "        print(f\"Number of unigrams: {len(self.unigram_dict)}\")\n",
    "        print(f\"Number of bigrams: {len(self.bigram_dict)}\")\n",
    "        print(f\"Number of trigrams: {len(self.trigram_dict)}\")\n",
    "    \n",
    "    def make_ngram_dicts(self):\n",
    "        \"\"\"\n",
    "        Create dictionaries for unigrams, bigrams, and trigrams\n",
    "        \"\"\"\n",
    "        for sentence in self.prepared_data:\n",
    "            for i in range(len(sentence)):\n",
    "                self.unigram_dict[sentence[i]] += 1\n",
    "                if i < len(sentence) - 1:    # if i > 0:\n",
    "                    self.bigram_dict[sentence[i-1]][sentence[i]] += 1\n",
    "                if i < len(sentence) - 2:    # if i > 1:\n",
    "                    self.trigram_dict[(sentence[i-2], sentence[i-1])][sentence[i]] += 1\n",
    "    \n",
    "    def calculate_smoothed_probs(self, n, input_text):\n",
    "        \"\"\"\n",
    "        Calculate the smoothed probabilities of the next word\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            n (int): The n-gram model to use\n",
    "            input_text (str): The input text\n",
    "        Returns:\n",
    "            probs (dict): A dictionary of the probabilities of the next word\n",
    "        \"\"\"\n",
    "        words = input_text.split()\n",
    "        probs = {}\n",
    "\n",
    "        for candidate in self.unigram_dict.keys():\n",
    "            word = candidate\n",
    "            if n == 2:\n",
    "                prev_word = words[-1]\n",
    "                probs[word] = (self.bigram_dict[prev_word][word] + 1 / (self.unigram_dict[prev_word] + len(self.unigram_dict)))\n",
    "            elif n == 3:\n",
    "                prev_bigram = (words[-2], words[-1])\n",
    "                probs[word] = (self.trigram_dict[prev_bigram][word] + 1 / (self.bigram_dict[prev_bigram[1]][word] + len(self.bigram_dict)))\n",
    "\n",
    "        return probs\n",
    "    \n",
    "    def generate_text(self, n, input_text=None, max_length=20):\n",
    "        \"\"\"\n",
    "        Generate text using the n-gram model\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            n (int): The n-gram model to use\n",
    "            input_text (str): The input text\n",
    "            max_length (int): The maximum length of the generated text\n",
    "        Returns:\n",
    "            text (str): The generated text\n",
    "        \"\"\"\n",
    "        if not input_text:\n",
    "            input_text = \" \".join(np.random.choice(list(self.unigram_dict.keys()), 3))\n",
    "            \n",
    "        text = input_text.split()\n",
    "        \n",
    "        print(input_text, end=' ')\n",
    "        for _ in range(max_length):\n",
    "            time.sleep(0.01)\n",
    "            probs = self.calculate_smoothed_probs(n, \" \".join(text))\n",
    "            # probs = self.kneser_ney_smoothing(2, \" \".join(text))\n",
    "            if not probs:\n",
    "                break\n",
    "            next_word = max(probs, key=probs.get)\n",
    "            text.append(next_word)\n",
    "            print(next_word, end=' ')\n",
    "        \n",
    "        return \" \".join(text)\n",
    "    \n",
    "    def evaluate_model(self, prepared_test_data, n):\n",
    "        \"\"\"\n",
    "        Evaluate the model using the test data\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            prepared_test_data (list): The preprocessed test data\n",
    "            n (int): The n-gram model to use\n",
    "        Returns:\n",
    "            log_likelihood (float): The log-likelihood of the model\n",
    "            perplexity (float): The perplexity of the model\n",
    "        \"\"\"\n",
    "        log_likelihood = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for sentence in prepared_test_data[:300]:\n",
    "            if len(sentence) < 3:\n",
    "                continue\n",
    "            input_text = \" \".join(sentence)\n",
    "            probs = self.calculate_smoothed_probs(n, input_text)\n",
    "            \n",
    "            for p in probs.values():\n",
    "                log_likelihood += np.log(p)\n",
    "                total_tokens += 1\n",
    "        \n",
    "        avg_log_likelihood = log_likelihood / total_tokens if total_tokens > 0 else float('-inf')\n",
    "        perplexity = np.exp(-avg_log_likelihood)\n",
    "        return avg_log_likelihood, perplexity\n",
    "    \n",
    "    def kneser_ney_smoothing(self, n, input_text, d=0.75):\n",
    "        \"\"\"\n",
    "        Calculate the Kneser-Ney smoothed probabilities of the next word\n",
    "        ----------------------------------------------\n",
    "        Args:\n",
    "            n (int): The n-gram model to use\n",
    "            input_text (str): The input text\n",
    "            d (float): The discount factor\n",
    "        Returns:\n",
    "            probs (list): A list of the probabilities of the next word\n",
    "        \"\"\"\n",
    "        words = input_text.split()\n",
    "        probs = {}\n",
    "\n",
    "        for candidate in self.unigram_dict.keys():\n",
    "            if candidate in words:\n",
    "                continue\n",
    "            word = candidate\n",
    "            context = \" \".join(words[-n+1:])\n",
    "            context_words = context.split()\n",
    "            context_word_counts = {}\n",
    "            for i in range(1, n):\n",
    "                context_word_counts[context_words[-i]] = len(self.bigram_dict[context_words[-i]])\n",
    "            if n == 2:\n",
    "                probs[word] = max(self.bigram_dict[context_words[-1]][word] - d, 0) / self.unigram_dict[context_words[-1]] + d * len(self.bigram_dict[context_words[-1]]) / self.unigram_dict[context_words[-1]] * self.bigram_dict[context_words[-1]][word]\n",
    "            else:\n",
    "                probs[word] = 0\n",
    "        \n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 567159\n",
      "Number of unique tokens: 18944\n"
     ]
    }
   ],
   "source": [
    "# tasnim_processor = DataProcessor('./Datasets/_CSV/tasnim_cleaned.csv')\n",
    "shahname_processor = DataProcessor('./Datasets/_CSV/shahname_cleaned.csv')\n",
    "# hamshahri_processor = DataProcessor('./Datasets/_CSV/hamshahri_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepapred_data = shahname_processor.prepared_data\n",
    "train_size = int(0.95 * len(prepapred_data))\n",
    "train_data = prepapred_data[:train_size]\n",
    "test_data = prepapred_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams: 9984\n",
      "Number of bigrams: 9700\n",
      "Number of trigrams: 143636\n"
     ]
    }
   ],
   "source": [
    "lm = NgramLanguageModel(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "به یاد ندارد جهان چون شب تیره \n",
      "شب تیره شد آن نامور گذر کرد \n",
      "گذر کرد بر خویشتن تار به خداوند \n",
      "به خداوند به خداوند به خداوند به \n"
     ]
    }
   ],
   "source": [
    "input_text = \"به یاد\"\n",
    "print(\"Generated text:\")\n",
    "# generated_text = lm.generate_text(3, input_text=None, max_length=20)\n",
    "generated_text = lm.generate_text(3, input_text, max_length=5)\n",
    "print()\n",
    "for _ in range(3):\n",
    "    generated_text = lm.generate_text(3, input_text=(generated_text.split()[-2] + ' ' + generated_text.split()[-1]), max_length=5)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model (n=3):\n",
      "Log likelihood: -9.170771065368235\n",
      "Perplexity: 9612.03333939397\n",
      "Evaluating the model (n=2):\n",
      "Log likelihood: -9.06635798044812\n",
      "Perplexity: 8659.029951535555\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the model (n=3):\")\n",
    "log_likelihood, perplexity = lm.evaluate_model(test_data, 3)\n",
    "print(f\"Log likelihood: {log_likelihood}\")\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print()\n",
    "print(\"Evaluating the model (n=2):\")\n",
    "log_likelihood, perplexity = lm.evaluate_model(test_data, 2)\n",
    "print(f\"Log likelihood: {log_likelihood}\")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kneser-Ney Smoothing:\n",
      "نزدیک: 27.301329740495575\n",
      "پیش: 23.0166071114756\n",
      "هر: 18.237493409876397\n",
      "کردار: 15.271146974401029\n",
      "سر: 13.89783843945873\n",
      "Add-1 Smoothing:\n",
      "چاره: 1.0001029442042413\n",
      "از: 1.0001026377912348\n",
      "درگاه: 1.0001020199959192\n",
      "جنگ: 1.0001017501017502\n",
      "ایران: 1.000100969305331\n"
     ]
    }
   ],
   "source": [
    "# Kneser-Ney Smoothing\n",
    "print(\"Kneser-Ney Smoothing:\")\n",
    "input_text = \"چو رستم بیاید به\"\n",
    "probs = lm.kneser_ney_smoothing(2, input_text)\n",
    "\n",
    "# Print top 5 probabilities (sorted by value)\n",
    "sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, prob in sorted_probs[:5]:\n",
    "    print(f\"{word}: {prob}\")\n",
    "\n",
    "# Smoothing\n",
    "print(\"Add-1 Smoothing:\")\n",
    "probs = lm.calculate_smoothed_probs(3, input_text)\n",
    "\n",
    "# Print top 5 probabilities (sorted by value)\n",
    "sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, prob in sorted_probs[:5]:\n",
    "    print(f\"{word}: {prob}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
